# AetherCampus

AetherCampus is a local-first AI platform built for campus environments.
It ships with a polished frontend, a Rust (`actix-web`) backend, and local model runtime support (Ollama), with optional cloud fallback.

## What You Get

- Local-first AI chat workspace
- Utility generation tools
- Responsible AI report dashboard
- Smart model routing (`fast`, `balanced`, `quality`)
- Auto setup for low-end/high-end machines
- Optional cloud escalation for hard queries

## Project Structure

```text
campus-ai-hub/
  apps/
    api-rust/                # Rust backend (Actix Web)
  infra/
    docker-compose.runtime.yml
    nginx.conf
    runtime.env              # generated by ./setup
  data/
    models/                  # local Ollama model storage
  src/                       # frontend
  setup                      # machine-aware setup script
```

## Quick Start (Recommended)

### 1) Prerequisites

- Docker + Docker Compose
- Node.js 18+ (for frontend local dev only)

### 2) Start the full stack

```bash
./setup
```

This command:

- detects hardware (CPU/RAM)
- chooses local/cloud mode automatically
- selects + pulls local model pack (in local mode)
- writes optimized runtime config to `infra/runtime.env`
- starts containers

### 3) Open app and verify

- Web UI: `http://localhost:8080`
- Health: `http://localhost:8000/health`
- Ready: `http://localhost:8000/ready`
- Metrics: `http://localhost:8000/metrics`

## Friendly Interactive Guide

If you want a fun terminal walkthrough for teammates/demo judges, run:

```bash
./guide
```

What `guide` helps with:

- visual overview of system architecture
- one-click setup mode selection
- health/readiness/metrics checks
- sample streaming chat API test
- quick access to docs and final runbook

## Setup Modes

```bash
./setup --auto      # default; hardware-aware mode selection
./setup --local     # force full local/offline model runtime
./setup --cloud     # force cloud mode
./setup --no-start  # generate config only, do not start containers
```

## Local Model Strategy

`setup` uses machine tiers and model pools:

- `low`: `qwen2.5:1.5b`, `phi3:mini`, `llama3.2:1b`
- `mid`: `qwen2.5:3b`, `llama3.2:3b`, `gemma2:2b`
- `high`: `qwen2.5:7b`, `llama3.1:8b`, `gemma2:9b`

Pack size by machine:

- `low` => 1 model
- `mid` => 2 models
- `high` => 3 models

Routing tiers in runtime env:

- `LOCAL_MODEL_FAST`
- `LOCAL_MODEL_BALANCED`
- `LOCAL_MODEL_QUALITY`

## Runtime Endpoints (Backend)

- `GET /health`
- `GET /ready`
- `GET /metrics`
- `POST /api/chat` (streaming)
- `GET /api/utility/templates`
- `POST /api/utility/generate`
- `GET /api/ai/report`

## Operations Guide

### View logs

```bash
docker compose -f infra/docker-compose.runtime.yml logs -f
```

### Restart stack

```bash
docker compose -f infra/docker-compose.runtime.yml down
docker compose -f infra/docker-compose.runtime.yml --profile local up -d --build
```

### Force cloud mode

```bash
./setup --cloud
```

Then set `CLOUD_API_KEY` in `infra/runtime.env` and restart containers.

### Stop everything

```bash
docker compose -f infra/docker-compose.runtime.yml down
```

## Frontend Routes

- `/` landing page
- `/workspace` chat
- `/utility-builder`
- `/responsible-ai`
- `/appearance`
- `/runtime-settings`

## Developer Commands

```bash
npm install
npm run dev
npm run build
npm run test
```

## Troubleshooting

- `ready` fails in local mode:
  - check Ollama container is running and healthy
  - verify models exist (`docker exec campus-local-model ollama list`)
- Slow response on low-end machine:
  - keep local mode, use smaller model tier
  - reduce context (`LOCAL_NUM_CTX`) in `infra/runtime.env`
- Cloud mode not working:
  - set `CLOUD_API_KEY`
  - verify `CLOUD_API_BASE_URL` and `CLOUD_MODEL`

## Final Documentation Pack

For a presentation-ready final guide:

- Markdown runbook: `docs/FINAL_RUNBOOK.md`
- Print-ready HTML: `docs/FINAL_RUNBOOK.html`

Open `docs/FINAL_RUNBOOK.html` in a browser and use **Print -> Save as PDF**.
