# Local Model Runtime

This folder is for your local inference runtime setup.

Recommended options:

- Ollama for quick local setup
- vLLM for higher throughput on GPU servers
- llama.cpp for lightweight CPU/GPU deployments

Use `infra/docker-compose.local.yml` to run a local model container and the API service together.
