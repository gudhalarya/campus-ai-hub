#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
RUNTIME_ENV="$ROOT_DIR/infra/runtime.env"
COMPOSE_FILE="$ROOT_DIR/infra/docker-compose.runtime.yml"
LOCAL_MODEL_SERVICE="local-model"
LOCAL_MODEL_CONTAINER="campus-local-model"
DOCKER_COMPOSE_CMD=()

MODE="auto"
START_STACK=1

usage() {
  cat <<USAGE
Usage: ./setup [--auto|--local|--cloud] [--no-start]

Options:
  --auto      Detect machine and auto-select mode (default)
  --local     Force local/offline runtime mode
  --cloud     Force cloud/API runtime mode
  --no-start  Only generate config, do not start containers
  -h, --help  Show this help
USAGE
}

while [[ $# -gt 0 ]]; do
  case "$1" in
    --auto) MODE="auto" ;;
    --local) MODE="local" ;;
    --cloud) MODE="cloud" ;;
    --no-start) START_STACK=0 ;;
    -h|--help) usage; exit 0 ;;
    *)
      echo "Unknown argument: $1" >&2
      usage
      exit 1
      ;;
  esac
  shift
done

have_cmd() {
  command -v "$1" >/dev/null 2>&1
}

resolve_compose_cmd() {
  if docker compose version >/dev/null 2>&1; then
    DOCKER_COMPOSE_CMD=(docker compose)
    return 0
  fi

  if have_cmd docker-compose; then
    DOCKER_COMPOSE_CMD=(docker-compose)
    return 0
  fi

  return 1
}

detect_cores() {
  if have_cmd nproc; then
    nproc
    return
  fi

  if [[ "$(uname -s)" == "Darwin" ]]; then
    sysctl -n hw.logicalcpu
    return
  fi

  echo 2
}

detect_ram_mb() {
  if [[ -r /proc/meminfo ]]; then
    awk '/MemTotal/ {print int($2/1024)}' /proc/meminfo
    return
  fi

  if [[ "$(uname -s)" == "Darwin" ]]; then
    local bytes
    bytes="$(sysctl -n hw.memsize)"
    echo $((bytes / 1024 / 1024))
    return
  fi

  echo 4096
}

machine_class() {
  local cores="$1"
  local ram_mb="$2"

  if (( cores < 2 || ram_mb < 4096 )); then
    echo "extreme"
  elif (( cores < 4 || ram_mb < 8192 )); then
    echo "low"
  elif (( cores < 8 || ram_mb < 16384 )); then
    echo "mid"
  else
    echo "high"
  fi
}

model_candidates_for_class() {
  local class="$1"

  case "$class" in
    low)
      echo "qwen2.5:1.5b phi3:mini llama3.2:1b"
      ;;
    mid)
      echo "qwen2.5:3b llama3.2:3b gemma2:2b"
      ;;
    high)
      echo "qwen2.5:7b llama3.1:8b gemma2:9b"
      ;;
    *)
      echo ""
      ;;
  esac
}

model_pack_size_for_class() {
  local class="$1"
  case "$class" in
    low) echo 1 ;;
    mid) echo 2 ;;
    high) echo 3 ;;
    *) echo 1 ;;
  esac
}

recommend_threads() {
  local cores="$1"
  if (( cores <= 2 )); then
    echo 1
  elif (( cores <= 4 )); then
    echo 2
  elif (( cores <= 8 )); then
    echo 4
  else
    echo 6
  fi
}

recommend_parallel() {
  local class="$1"
  case "$class" in
    low|mid) echo 1 ;;
    high) echo 2 ;;
    *) echo 1 ;;
  esac
}

write_runtime_env() {
  local resolved_mode="$1"
  local ollama_model="$2"
  local local_fast="$3"
  local local_balanced="$4"
  local local_quality="$5"
  local ollama_threads="$6"
  local ollama_parallel="$7"
  local arch="$8"
  local class="$9"
  local cores="${10}"
  local ram_mb="${11}"

  cat > "$RUNTIME_ENV" <<ENV
# Auto-generated by ./setup on $(date -u +"%Y-%m-%dT%H:%M:%SZ")
MODE=$resolved_mode
PORT=8000
LOCAL_MODEL_BASE_URL=http://local-model:11434
OLLAMA_MODEL=${ollama_model}
LOCAL_MODEL_FAST=${local_fast}
LOCAL_MODEL_BALANCED=${local_balanced}
LOCAL_MODEL_QUALITY=${local_quality}
OLLAMA_NUM_THREADS=${ollama_threads}
OLLAMA_NUM_PARALLEL=${ollama_parallel}
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_KEEP_ALIVE=2m
SMART_ROUTING=true
CLOUD_ESCALATION=false
MAX_INPUT_CHARS=12000
LOCAL_TEMPERATURE=0.2
LOCAL_TOP_P=0.9
LOCAL_NUM_CTX=4096
RESPONSE_CACHE_SIZE=120
RESPONSE_CACHE_TTL_SECONDS=900
UPSTREAM_TIMEOUT_MS=90000
CLOUD_API_BASE_URL=https://api.openai.com/v1
CLOUD_MODEL=gpt-4.1-mini
# Set before cloud runs:
CLOUD_API_KEY=${CLOUD_API_KEY:-}

# Detection metadata
MACHINE_ARCH=${arch}
MACHINE_CLASS=${class}
CPU_CORES=${cores}
RAM_MB=${ram_mb}
ENV
}

wait_for_ollama() {
  local max_tries=50
  local try=1

  while (( try <= max_tries )); do
    if docker exec "$LOCAL_MODEL_CONTAINER" ollama list >/dev/null 2>&1; then
      return 0
    fi

    sleep 2
    try=$((try + 1))
  done

  return 1
}

pull_best_local_model() {
  local candidates="$1"

  for model in $candidates; do
    echo "Trying local model: $model"
    if docker exec "$LOCAL_MODEL_CONTAINER" ollama pull "$model"; then
      echo "$model"
      return 0
    fi
    echo "Failed to pull model: $model (trying next)"
  done

  return 1
}

pull_model_pack() {
  local candidates="$1"
  local limit="$2"
  local pulled=()
  local count=0

  for model in $candidates; do
    if (( count >= limit )); then
      break
    fi

    echo "Trying local model: $model"
    if docker exec "$LOCAL_MODEL_CONTAINER" ollama pull "$model"; then
      pulled+=("$model")
      count=$((count + 1))
      echo "Pulled model: $model"
    else
      echo "Failed to pull model: $model (trying next)"
    fi
  done

  echo "${pulled[*]}"
}

CORES="$(detect_cores)"
RAM_MB="$(detect_ram_mb)"
ARCH="$(uname -m)"
CLASS="$(machine_class "$CORES" "$RAM_MB")"

if [[ "$MODE" == "auto" ]]; then
  if [[ "$CLASS" == "extreme" ]]; then
    RESOLVED_MODE="cloud"
  else
    RESOLVED_MODE="local"
  fi
else
  RESOLVED_MODE="$MODE"
fi

MODEL_CANDIDATES="$(model_candidates_for_class "$CLASS")"
MODEL_PACK_SIZE="$(model_pack_size_for_class "$CLASS")"
DEFAULT_OLLAMA_MODEL=""
if [[ -n "$MODEL_CANDIDATES" ]]; then
  DEFAULT_OLLAMA_MODEL="${MODEL_CANDIDATES%% *}"
fi

LOCAL_MODEL_FAST="$DEFAULT_OLLAMA_MODEL"
LOCAL_MODEL_BALANCED="$DEFAULT_OLLAMA_MODEL"
LOCAL_MODEL_QUALITY="$DEFAULT_OLLAMA_MODEL"

if [[ -n "$MODEL_CANDIDATES" ]]; then
  set -- $MODEL_CANDIDATES
  LOCAL_MODEL_FAST="${1:-$DEFAULT_OLLAMA_MODEL}"
  LOCAL_MODEL_BALANCED="${2:-$LOCAL_MODEL_FAST}"
  LOCAL_MODEL_QUALITY="${3:-$LOCAL_MODEL_BALANCED}"
fi

if [[ "$RESOLVED_MODE" == "cloud" ]]; then
  DEFAULT_OLLAMA_MODEL=""
  LOCAL_MODEL_FAST=""
  LOCAL_MODEL_BALANCED=""
  LOCAL_MODEL_QUALITY=""
fi

OLLAMA_NUM_THREADS="$(recommend_threads "$CORES")"
OLLAMA_NUM_PARALLEL="$(recommend_parallel "$CLASS")"

mkdir -p "$ROOT_DIR/data/models"
write_runtime_env "$RESOLVED_MODE" "$DEFAULT_OLLAMA_MODEL" "$LOCAL_MODEL_FAST" "$LOCAL_MODEL_BALANCED" "$LOCAL_MODEL_QUALITY" "$OLLAMA_NUM_THREADS" "$OLLAMA_NUM_PARALLEL" "$ARCH" "$CLASS" "$CORES" "$RAM_MB"

echo ""
echo "Machine detection summary"
echo "  Architecture : $ARCH"
echo "  CPU cores    : $CORES"
echo "  RAM (MB)     : $RAM_MB"
echo "  Class        : $CLASS"
echo "  Mode         : $RESOLVED_MODE"
if [[ "$RESOLVED_MODE" == "local" ]]; then
  echo "  Model pool   : ${MODEL_CANDIDATES:-none}"
  echo "  Pack size    : $MODEL_PACK_SIZE"
  echo "  Threads      : $OLLAMA_NUM_THREADS"
  echo "  Parallel req : $OLLAMA_NUM_PARALLEL"
fi

echo ""
echo "Wrote optimized runtime config: $RUNTIME_ENV"

if (( START_STACK == 0 )); then
  echo "Skipping container startup (--no-start)."
  exit 0
fi

if ! have_cmd docker; then
  echo "Docker not found. Install Docker and run:"
  echo "  docker compose -f infra/docker-compose.runtime.yml --profile $RESOLVED_MODE up -d --build"
  exit 0
fi

if ! docker info >/dev/null 2>&1; then
  echo "Docker daemon is not available. Start Docker and then run:"
  echo "  docker compose -f infra/docker-compose.runtime.yml --profile $RESOLVED_MODE up -d --build"
  exit 0
fi

if ! resolve_compose_cmd; then
  echo "Docker Compose not found."
  echo "Install Docker Compose v2 plugin or docker-compose and re-run setup."
  exit 1
fi

if [[ "$RESOLVED_MODE" == "local" ]]; then
  echo ""
  echo "Starting local model container for auto model pull..."
  "${DOCKER_COMPOSE_CMD[@]}" -f "$COMPOSE_FILE" --profile local up -d "$LOCAL_MODEL_SERVICE"

  if wait_for_ollama; then
    SELECTED_MODEL=""
    if [[ -n "$MODEL_CANDIDATES" ]]; then
      PULLED_MODELS="$(pull_model_pack "$MODEL_CANDIDATES" "$MODEL_PACK_SIZE")"
      set -- $PULLED_MODELS
      if [[ -n "${1:-}" ]]; then
        SELECTED_MODEL="${1:-$DEFAULT_OLLAMA_MODEL}"
        LOCAL_MODEL_FAST="${1:-$SELECTED_MODEL}"
        LOCAL_MODEL_BALANCED="${2:-$LOCAL_MODEL_FAST}"
        LOCAL_MODEL_QUALITY="${3:-$LOCAL_MODEL_BALANCED}"
        echo "Selected local model pack:"
        echo "  fast     : $LOCAL_MODEL_FAST"
        echo "  balanced : $LOCAL_MODEL_BALANCED"
        echo "  quality  : $LOCAL_MODEL_QUALITY"
        write_runtime_env "$RESOLVED_MODE" "$SELECTED_MODEL" "$LOCAL_MODEL_FAST" "$LOCAL_MODEL_BALANCED" "$LOCAL_MODEL_QUALITY" "$OLLAMA_NUM_THREADS" "$OLLAMA_NUM_PARALLEL" "$ARCH" "$CLASS" "$CORES" "$RAM_MB"
      else
        echo "Warning: no model pull succeeded. Setup will continue with configured fallback."
      fi
    fi
  else
    echo "Warning: local model runtime did not become ready in time."
  fi
fi

echo ""
echo "Starting optimized container stack..."
"${DOCKER_COMPOSE_CMD[@]}" -f "$COMPOSE_FILE" --profile "$RESOLVED_MODE" up -d --build

echo ""
echo "Stack started."
echo "  Web: http://localhost:8080"
echo "  API: http://localhost:8000/health"
if [[ "$RESOLVED_MODE" == "local" ]]; then
  echo "  Model runtime: http://localhost:11434"
  echo "  Active local model: $(awk -F= '/^OLLAMA_MODEL=/ {print $2}' "$RUNTIME_ENV")"
  echo "  Routing tiers:"
  echo "    fast     : $(awk -F= '/^LOCAL_MODEL_FAST=/ {print $2}' "$RUNTIME_ENV")"
  echo "    balanced : $(awk -F= '/^LOCAL_MODEL_BALANCED=/ {print $2}' "$RUNTIME_ENV")"
  echo "    quality  : $(awk -F= '/^LOCAL_MODEL_QUALITY=/ {print $2}' "$RUNTIME_ENV")"
else
  echo "  Cloud mode enabled. Set CLOUD_API_KEY in infra/runtime.env if not set."
fi
